{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73143ab-0eed-4592-80bf-d4a1a1f02ed8",
   "metadata": {},
   "source": [
    "# Part 2 Transformer (Text generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa2e65c-3332-483d-8aca-1ec0c7a4f6af",
   "metadata": {},
   "source": [
    "## Setting Up PyTorch Device for GPU or CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec91aa54-6015-4fff-acf6-abba3aa92022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'using device {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94de012-161d-48ca-a91d-db835f8179a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = 'data'\n",
    "output_file = 'text_generation.txt'\n",
    "\n",
    "def is_hidden(filepath):\n",
    "    pass\n",
    "\n",
    "with open(output_file, 'w') as outfile:\n",
    "    for filename in os.listdir(data_dir):\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "        if os.path.isfile(filepath):  # Check if it's a file\n",
    "            if not is_hidden(filepath):\n",
    "                with open(filepath) as infile:\n",
    "                    for line in infile:\n",
    "                        if line.strip():\n",
    "                            outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1bec380-c82b-4823-ae67-94ea651b4f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-25 00:28:43.475020: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-25 00:28:43.663937: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-25 00:28:43.664003: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-25 00:28:43.697904: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-25 00:28:43.761794: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-25 00:28:44.774998: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# i use GPT-2-medium\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium') \n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "\n",
    "# set padding token to assure consistent sequence lengths  \n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b8a686-350c-4cd1-aa83-b3f61525fa5e",
   "metadata": {},
   "source": [
    "## Create a class to Fine-Tuning with custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04d10c9c-81a7-41f2-9e21-e45cd7b1238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenizer, file_path, block_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        with open(file_path, \"r\") as f:\n",
    "            self.text = f.read().splitlines()\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_input = self.tokenizer(\n",
    "            self.text[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length = 128,\n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "        tokenized_input['labels'] = tokenized_input['input_ids']\n",
    "        return tokenized_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c1beec-f019-4959-bec4-aa6ca59f7e17",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bb0f644-4116-4309-b71e-1583a90326fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CustomDataset(tokenizer, \"text_generation.txt\", 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f78278d-ecb9-4cbd-a2bf-761048fc421b",
   "metadata": {},
   "source": [
    "## Create a data collator that will dynamically pad the sequences\n",
    "    Badged data to the maximum sequence lenght in a batch it pads each batch to the length of the longest sequence in that batch saving processing complexities by passing the tokenizer to data collator with padding we assure that padding is done correctly according to the tokenizer setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eda67c02-537a-4d78-8cba-5bd575666e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bde650f-9b67-4f60-a0c8-35f389682196",
   "metadata": {},
   "source": [
    "## Training arguments and trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "605f001b-bddb-4b8c-b4bc-0d9a5d568235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hafdaoui/.local/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size = 4,\n",
    "    num_train_epochs = 10,\n",
    "    learning_rate = 1e-4,\n",
    "    output_dir = './results',\n",
    "    logging_dir = './logs',\n",
    "    logging_steps = 10,\n",
    "    load_best_model_at_end = False,\n",
    "    evaluation_strategy=\"no\",\n",
    "    remove_unused_columns = False,\n",
    "    push_to_hub= False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "832f3e3e-2088-4749-a1c3-6a22f0bdd04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args=training_args,\n",
    "    train_dataset = data,\n",
    "    eval_dataset= None,\n",
    "    data_collator = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d012f525-5ad7-45ce-b35b-3ad2e07b8f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 39:56, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.826000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.760100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.496300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.346400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.300400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.182500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.181700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.138500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.088900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.067400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.101700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.066300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.061500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.048600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.040200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.036800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.030900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.036300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.028300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.033400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.029600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=0.20630556225776672, metrics={'train_runtime': 2407.3611, 'train_samples_per_second': 0.403, 'train_steps_per_second': 0.104, 'total_flos': 225209918423040.0, 'train_loss': 0.20630556225776672, 'epoch': 10.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b526e150-60d5-449c-a36e-f3164416d9c1",
   "metadata": {},
   "source": [
    "# Save the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "858f60dc-56c4-4a63-88fa-420afa821b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_model/tokenizer_config.json',\n",
       " './saved_model/special_tokens_map.json',\n",
       " './saved_model/vocab.json',\n",
       " './saved_model/merges.txt',\n",
       " './saved_model/added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./saved_model')\n",
    "tokenizer.save_pretrained('./saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9cabd1-fcb1-4f39-86b8-ccddc487f23c",
   "metadata": {},
   "source": [
    "# Now i will Test GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfeb8ebe-fa2b-41c9-b68d-9e9017c4aa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=200):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        temperature=1.5,\n",
    "        num_beams=5,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c606068-e246-46b9-8ecc-7d88815b3c15",
   "metadata": {},
   "source": [
    "## Gradio Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d41234b-8261-46aa-abf6-8c12f9afb884",
   "metadata": {},
   "source": [
    "## Load the saved model and tokenizer before launching the Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0f2e68b-12ac-4682-904b-a63acdfa1505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio interface\n",
    "import gradio as gr\n",
    "\n",
    "def gradio_generate_text(prompt):\n",
    "    return generate_text(prompt)\n",
    "\n",
    "# Load the saved model and tokenizer before launching the Gradio interface\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./saved_model')\n",
    "model = GPT2LMHeadModel.from_pretrained('./saved_model')\n",
    "model.to(device)\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_generate_text, \n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your prompt here...\"), \n",
    "    outputs=gr.Textbox(lines=10), \n",
    "    title=\"GPT-2 Text Generation\",\n",
    "    description=\"Enter a prompt and the model will generate a continuation.\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b8b2dc-89db-413c-8d6f-782e9f71605f",
   "metadata": {},
   "source": [
    "# Launch the interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "044863ea-0652-4735-9805-09e3dea2bee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f920eaf-69c1-4c95-9ae0-0e832259cc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hafdaoui/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " the Kingdom of Morocco is a country located in North Africa. It is bordered by the Atlantic Ocean and the Mediterranean Sea to the west and north, Algeria to the east, and Western Sahara to the south.\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, max_length=50, temperature=0.7):\n",
    "    # Tokenize the input prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    # Generate text\n",
    "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, temperature=temperature, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # Decode the generated text and return it\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Refine the prompt strategy\n",
    "prompt = \"the Kingdom of Morocco\"\n",
    "generated_text = generate_text(prompt, max_length=150, temperature=0.8)\n",
    "print(\"Generated Text:\\n\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "975f1006-098b-4ef8-b500-91c670f16271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " moroccan festivals such as the Festival of World Sacred Music in Fez, the Mawazine World Music Festival in Rabat, and the Gnaoua World Music Festival in Essaouira attract artists and performers from around the world.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"moroccan festivals\"\n",
    "generated_text = generate_text(prompt, max_length=150, temperature=0.8)\n",
    "print(\"Generated Text:\\n\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00905f51-8551-457d-87fa-6e5cfe27d3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " explain to me how people live in morocco, a country with a rich history and cultural heritage that continues to attract visitors from around the world. From bustling medinas to tranquil rural villages, Moroccans lead lives shaped by centuries of history, religion, and geography.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"explain to me how people live in morocco\"\n",
    "generated_text = generate_text(prompt, max_length=150, temperature=0.8)\n",
    "print(\"Generated Text:\\n\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "831b7786-830d-4a84-bf27-4e9594370df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " Moroccan football traces its roots back to the early 20th century when the sport was introduced by European colonizers. The Moroccan Football Federation (FRMF) was founded in 1955, marking a significant milestone in the development of organized football in the country.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Moroccan football\"\n",
    "generated_text = generate_text(prompt, max_length=150, temperature=0.8)\n",
    "print(\"Generated Text:\\n\", generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
